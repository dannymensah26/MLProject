{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Information Theory in Machine Learning\n",
    "\n",
    "Welcome to this week's lab on Information Theory! This week, we will dive into the fascinating world of Information Theory as applied to Machine Learning. Specifically, we will focus on two key concepts: Entropy and Information Gain. These principles are fundamental in understanding how decision trees make split decisions to organize data effectively.\n",
    "\n",
    "### Entropy\n",
    "- Entropy, in the context of information theory, measures the level of uncertainty or disorder within a set of data.\n",
    "- In machine learning, particularly in decision trees, entropy helps to determine how a dataset should be split. A high entropy means more disorder, indicating that our dataset is varied. Conversely, low entropy suggests more uniformity in the data.\n",
    "\n",
    "### Information Gain\n",
    "- Information Gain measures the reduction in entropy after the dataset is split on an attribute.\n",
    "- It is crucial in building decision trees as it helps to decide the order of attributes the tree will use for splitting the data. The attribute with the highest Information Gain is chosen as the splitting attribute at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Entropy and Information Gain in Decision Trees\n",
    "Decision Trees use these concepts to create branches. By choosing splits that maximize Information Gain (or equivalently minimize entropy), a decision tree can effectively categorize data, leading to better classification or regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load and Explore the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate Entropy\n",
    "To calculate the `entropy` we need to:\n",
    "- First, extract the target variable `y` from your dataset (like the 'target' column in the Iris dataset).\n",
    "- Then, call `calculate_entropy(y)` to get the entropy.\n",
    "\n",
    "This function calculates the entropy of a given target variable `y`. It works by first determining the unique classes in `y`, then computes the probability of each class, and uses this probability to calculate the entropy. This is a crucial step in understanding the disorder or uncertainty in the dataset, a fundamental concept in information theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Extract the target variable y from the iris dataset\n",
    "y = iris.target\n",
    "\n",
    "# Print the values of y\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    class_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in class_labels:\n",
    "        probability = len(y[y == label]) / len(y)\n",
    "        entropy -= probability * np.log2(probability)\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the target variable(y): 1.584963\n"
     ]
    }
   ],
   "source": [
    "# Call the calculate_entropy function on y\n",
    "entropy = calculate_entropy(y)\n",
    "\n",
    "# Print the calculated entropy\n",
    "print(f\"Entropy of the target variable(y): {entropy:4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is your observation about the calculated Entropy?\n",
    "\n",
    "In the Iris dataset, the target variable (species) has an entropy score of 1.584953, which indicates a moderate level of uncertainty. \n",
    "\n",
    "Entropy values range from 0 (perfectly ordered, no uncertainty) to higher values (increased uncertainty).\n",
    "\n",
    "In the context of a classification problem, where the target variable represents different species (setosa, versicolor, virginica), this entropy value indicates a moderate level of diversity among the species. \n",
    "\n",
    "It suggests that the distribution of instances across the classes is not perfectly uniform (low entropy) but also not extremely skewed or concentrated in a few classes (high entropy). \n",
    "\n",
    "In summary, the calculated entropy reflects a moderate level of uncertainty or diversity in the target variable classes of the Iris dataset. Interpretation can be more nuanced based on the specific objectives and characteristics of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate Information Gain\n",
    "There are three steps for calculating the Information Gain:\n",
    "1. Compute Overall Entropy: Use the entropy function from Step 3 on the entire target dataset.\n",
    "2. Calculate Weighted Entropy for Each Attribute: For each unique value in the attribute, partition the dataset and calculate its entropy. Then calculate the weighted sum of these entropies, where the weights are the proportions of instances in each partition.\n",
    "3. Compute Information Gain: Subtract the weighted entropy of the split from the original entropy.\n",
    "\n",
    "The attribute with the highest Information Gain is generally chosen for splitting, as it provides the most significant reduction in uncertainty. This step is critical in constructing an effective decision tree, as it directly influences the structure and depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(df, attribute, target_name):\n",
    "    total_entropy = calculate_entropy(df[target_name])\n",
    "    values, counts = np.unique(df[attribute], return_counts=True)\n",
    "    weighted_entropy = sum((counts[i] / sum(counts)) * calculate_entropy(df.where(df[attribute] == values[i]).dropna()[target_name]) for i in range(len(values)))\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for 'sepal length': 0.8769\n",
      "Information Gain for 'petal length': 1.4463\n",
      "Information Gain for 'sepal width': 0.5166\n",
      "Information Gain for 'petal width': 1.4359\n"
     ]
    }
   ],
   "source": [
    "iris_information_gain = calculate_information_gain(df, 'sepal length (cm)', 'target')\n",
    "print(f\"Information Gain for 'sepal length': {iris_information_gain:.4f}\")\n",
    "\n",
    "iris_information_gain = calculate_information_gain(df, 'petal length (cm)', 'target')\n",
    "print(f\"Information Gain for 'petal length': {iris_information_gain:.4f}\")\n",
    "\n",
    "iris_information_gain = calculate_information_gain(df, 'sepal width (cm)', 'target')\n",
    "print(f\"Information Gain for 'sepal width': {iris_information_gain:.4f}\")\n",
    "\n",
    "iris_information_gain = calculate_information_gain(df, 'petal width (cm)', 'target')\n",
    "print(f\"Information Gain for 'petal width': {iris_information_gain:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss your findings here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Apply Entropy and Information Gain on a different dataset\n",
    "\n",
    "Your task is to choose a new dataset and implement what you learned in `Part 1` on this new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement Entropy and Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load boston data\n",
    "diabetes = load_diabetes()\n",
    "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "df['target'] = diabetes.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n"
     ]
    }
   ],
   "source": [
    "# Extract the target variable y from the iris dataset\n",
    "y = diabetes.target\n",
    "\n",
    "# Print the values of y\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    class_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in class_labels:\n",
    "        probability = len(y[y == label]) / len(y)\n",
    "        entropy -= probability * np.log2(probability)\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the target variable(y): 7.541840\n"
     ]
    }
   ],
   "source": [
    "# Call the calculate_entropy function on y\n",
    "entropy = calculate_entropy(y)\n",
    "\n",
    "# Print the calculated entropy\n",
    "print(f\"Entropy of the target variable(y): {entropy:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(df, attribute, target_name):\n",
    "    total_entropy = calculate_entropy(df[target_name])\n",
    "    values, counts = np.unique(df[attribute], return_counts=True)\n",
    "    weighted_entropy = sum((counts[i] / sum(counts)) * calculate_entropy(df.where(df[attribute] == values[i]).dropna()[target_name]) for i in range(len(values)))\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Information Gain for 'age': 4.3541\n",
      " Information Gain for 'sex': 4.3541\n",
      " Information Gain for 'bmi': 5.8400\n",
      " Information Gain for 'bp': 4.7758\n",
      " Information Gain for 's1': 5.5924\n",
      " Information Gain for 's2': 6.8312\n",
      " Information Gain for 's3': 0.4839\n",
      " Information Gain for 's4': 2.4058\n",
      " Information Gain for 's5': 5.9613\n",
      " Information Gain for 's6': 4.2252\n"
     ]
    }
   ],
   "source": [
    "diabetes_information_gain = calculate_information_gain(df, 'age', 'target')\n",
    "print(f\" Information Gain for 'age': {diabetes_information_gain:.4f}\")\n",
    "\n",
    "breast_information_gain = calculate_information_gain(df, 'sex', 'target')\n",
    "print(f\" Information Gain for 'sex': {diabetes_information_gain:.4f}\")\n",
    "\n",
    "diabetes_information_gain = calculate_information_gain(df, 'bmi', 'target')\n",
    "print(f\" Information Gain for 'bmi': {diabetes_information_gain:.4f}\")\n",
    "\n",
    "diabetes_information_gain = calculate_information_gain(df, 'bp', 'target')\n",
    "print(f\" Information Gain for 'bp': {diabetes_information_gain:.4f}\")\n",
    "\n",
    "diabetes_information_gain = calculate_information_gain(df, 's1', 'target')\n",
    "print(f\" Information Gain for 's1': {diabetes_information_gain:.4f}\")\n",
    "\n",
    "diabetes_information_gain = calculate_information_gain(df, 's2', 'target')\n",
    "print(f\" Information Gain for 's2': {diabetes_information_gain:.4f}\")\n",
    "\n",
    "diabetes_information_gain = calculate_information_gain(df, 's3', 'target')\n",
    "print(f\" Information Gain for 's3': {breast_information_gain:.4f}\")\n",
    "\n",
    "\n",
    "diabetes_information_gain = calculate_information_gain(df, 's4', 'target')\n",
    "print(f\" Information Gain for 's4': {diabetes_information_gain:.4f}\")\n",
    "\n",
    "diabetes_information_gain = calculate_information_gain(df, 's5', 'target')\n",
    "print(f\" Information Gain for 's5': {diabetes_information_gain:.4f}\")\n",
    "\n",
    "diabetes_information_gain = calculate_information_gain(df, 's6', 'target')\n",
    "print(f\" Information Gain for 's6': {diabetes_information_gain:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Discuss your findings in detail\n",
    "Provide a detailed explanation and discussion about your findings.\n",
    "\n",
    "\n",
    "• The findings from this study show that the information gain of each feature varies depending on how much it reduces the uncertainty of the target variable. The information gain can be used as a criterion for selecting the best features for building a decision tree classifier, as it indicates how well a feature can split the data into homogeneous subsets. A detailed explanation and discussion of the findings are as follows:\n",
    "\n",
    "• The feature with the highest information gain is blood serum, s2, with a value of 6.8312. This means that the s2 feature provides the most information about whether a patient has diabetes or not, and it can best separate the patients into two groups based on their s2 levels. \n",
    "\n",
    "• The feature with the second highest information gain is blood serum s5 with a measurement of 5.9613. This means that the s5 feature also provides a lot of information about whether a patient has diabetes or not, and it can also split the data into two groups based on their s2 values. This is consistent with the biological knowledge that diabetes has a genetic component and that the blood serum is a measure of the likelihood of diabetes based on family history.\n",
    "\n",
    "• The feature with the third highest information gain is Body Mass Index (BMI), with a value of 0.157. This means that the BMI feature also provides some information about whether a patient has diabetes or not, and it can also split the data into two groups based on their BMI values. This is consistent with the epidemiological knowledge that diabetes is more prevalent among people with a higher BMI index.\n",
    "\n",
    "• The features with the lowest information gain are blood serums, s3, s3 and S6, with values of 0.4839, 2.4058, and 4.2252, respectively. This means that these features provide very little information about whether a patient has diabetes or not. This may be because these features are not directly related to diabetes, or because they have a lot of variability or missing values in the data.\n",
    "\n",
    "• These findings suggest that s2, s5, and BMI are the most important features for predicting diabetes, and they should be given higher priority when building a decision tree classifier. The blood serums, s3, s3 and S6 features are the least important features for predicting diabetes, and they should be given lower priority or excluded when building a decision tree classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Submit a link to your completed Jupyter Notebook file hosted on your private GitHub repository through the submission link in Blackboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
